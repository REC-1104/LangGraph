{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# Language Model (LLM) imports\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Embedding model from Google Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # Chat model from Google Gemini\n",
    "from langchain_groq import ChatGroq  # Chat model from Groq (alternative to OpenAI)\n",
    "\n",
    "# Chat memory management\n",
    "from langchain_core.chat_history import BaseChatMessageHistory  # Base class for storing chat history\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory  # Stores chat history in memory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # Enables message history in pipelines\n",
    "\n",
    "# RAG (Retrieval-Augmented Generation) with LCEL (LangChain Expression Language)\n",
    "from langchain_community.document_loaders import TextLoader  # Loads text from a single file\n",
    "from langchain_community.document_loaders import DirectoryLoader  # Loads multiple files from a directory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits text into smaller chunks\n",
    "from langchain_community.vectorstores import Chroma  # Stores embeddings and enables similarity search\n",
    "# Runnables for modular pipelines\n",
    "from langchain_core.runnables import RunnableParallel  # Runs multiple functions in parallel\n",
    "from langchain_core.runnables import RunnablePassthrough  # Passes input unchanged\n",
    "# Output parsing\n",
    "from langchain_core.output_parsers import StrOutputParser  # Converts model output into a clean string\n",
    "from langchain_core.prompts import PromptTemplate # Creates structured prompts for LLMs.\n",
    "\n",
    "# Tools\n",
    "from langchain_community.tools import WikipediaQueryRun  # Searches Wikipedia and retrieves summaries\n",
    "from langchain_community.tools import YouTubeSearchTool  # Searches YouTube for relevant videos\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # Provides an API wrapper for Wikipedia queries\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults  # Fetches search results from Tavily (a web search tool)\n",
    "\n",
    "# Agents\n",
    "from langchain.agents import AgentType  # Defines different types of agents (e.g., conversational, task-based)\n",
    "from langchain.agents import load_tools  # Loads tools that agents can use (e.g., search engines, APIs)\n",
    "from langchain.agents import initialize_agent  # Initializes an agent with tools and agent type for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m LANGCHAIN_API_KEY = \u001b[43mos\u001b[49m.getenv(\u001b[33m\"\u001b[39m\u001b[33mLANGCHAIN_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m GOOGLE_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m GROQ_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings  = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "llm_g = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_version=\"v1beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings  = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "llm = ChatGroq(model=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too afraid to test me huh !\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"Type your question. Please write 'quit' if you dont want to ask.\")\n",
    "    if question.lower() != 'quit' :\n",
    "        print(llm.invoke(question).content)\n",
    "        print()\n",
    "        print(\"If it not satisfies you, then I wont feel bad, as I'm just a 'Simple AI Assistant' \")\n",
    "        print('____________________________________________________________________________________________')\n",
    "    else:\n",
    "        print(\"Too afraid to test me huh !\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Dhruv, it's nice to meet you!\\n\\nWhat can I do for you today? ðŸ˜Š\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi, I am Dhruv\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and no access to personal information about you. Therefore, I don't know your name. ðŸ˜Š\\n\\nIf you'd like to tell me your name, I'd be happy to know!\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"tell me what's my name is ?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will not have the context of previous chat, thats why creating a memory for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can create new sessions like \"New Chat\" in chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"your_unique_session_id_here\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_memory = RunnableWithMessageHistory(llm,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Dhruv, it's nice to meet you!\\n\\nWhat can I do for you today? ðŸ˜Š\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_memory.invoke((\"Hi, I am Dhruv\"),config=config).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Dhruv, as you told me earlier!  ðŸ˜„  \\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_memory.invoke((\"tell me what's my name is ?\"),config=config).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'your_unique_session_id_here': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, I am Dhruv', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Dhruv, it's nice to meet you!\\n\\nWhat can I do for you today? ðŸ˜Š\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 16, 'total_tokens': 42, 'completion_time': 0.047272727, 'prompt_time': 0.001928877, 'queue_time': 0.230968622, 'total_time': 0.049201604}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7b6418c6-63ff-46ff-8134-1b87231b2da8-0', usage_metadata={'input_tokens': 16, 'output_tokens': 26, 'total_tokens': 42}), HumanMessage(content=\"tell me what's my name is ?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Dhruv, as you told me earlier!  ðŸ˜„  \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 59, 'total_tokens': 89, 'completion_time': 0.054545455, 'prompt_time': 0.003971433, 'queue_time': 0.233338408, 'total_time': 0.058516888}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-11013d8e-d891-4249-8ad5-e6cdbf6cbd25-0', usage_metadata={'input_tokens': 59, 'output_tokens': 30, 'total_tokens': 89})])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG WITH LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the source file from 'data' directory\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    '../data1',\n",
    "    glob=\"./*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='Llama (Large Language Model Meta AI) is a family of autoregressive large language models released by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April 2024.[4]\\n\\nModel weights for the first version of Llama were released to the research community under a non-commercial license.[5][3] Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use.[6][7] Llama models are trained at different parameter sizes, typically ranging between 7B and 70B.[4] Originally, Llama was only available as a foundation model.[8] Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.[7]\\n\\nLlama models have been compared favorably against other large language models. Meta AI reported the original 13B parameter model\\'s performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[2]. Meta AI\\'s testing shows that Llama 3 70B beats Gemini, and Claude in most benchmarks.[9][10] Wired describes the 8B parameter version of Llama 3 as being \"surprisingly capable\" given it\\'s size.[11]\\n\\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.[12] Reception was mixed, with some users confused after Meta AI told a parental group that it had a child.[13]')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 10,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='Llama (Large Language Model Meta AI) is a family'),\n",
       " Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='a family of autoregressive large language models'),\n",
       " Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='models released by Meta AI starting in February'),\n",
       " Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='February 2023.[2][3] The latest version is Llama'),\n",
       " Document(metadata={'source': '..\\\\data\\\\llama3.txt'}, page_content='is Llama 3 released in April 2024.[4]')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Llama (Large Language Model Meta AI) is a family',\n",
       " 'a family of autoregressive large language models',\n",
       " 'models released by Meta AI starting in February',\n",
       " 'February 2023.[2][3] The latest version is Llama',\n",
       " 'is Llama 3 released in April 2024.[4]']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating retriever using vector DB\n",
    "\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\":4}) # Retrieves the top 4 most similar document chunks for any query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\" : retriever, \"question\" : RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm_g\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text:\n",
      "\n",
      "1. **Llama 3 has an 8B parameter version.** This refers to the size of the model, indicating its complexity.\n",
      "2. **Llama 3 was (potentially) released in April 2024.**  The repeated phrase with the citation suggests a possible release date, but the text doesn't definitively confirm it.\n",
      "3. **The provided text fragments give a partial description of Llama 3 but don't fully define it.** We know some details, but not its overall purpose or capabilities.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is llama3? Can you highlight 3 important points\"\n",
    "print(retrieval_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools & Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper = WikipediaAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool1 = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool1.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'description': 'query to look up on wikipedia',\n",
       "  'title': 'Query',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool1.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information.  Use cases include providing chatbot access to internal company data or generating responses based on authoritative sources.\n",
      "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.\n",
      "By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to IBM, \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting.\"\n",
      "Beyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
      "\n",
      "Page: Milvus (vector database)\n",
      "Summary: Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\n",
      "Milvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\n"
     ]
    }
   ],
   "source": [
    "print(tool1.run({\"query\" : \"langchain\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2 = YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youtube_search'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool2.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool2.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query', 'type': 'string'}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool2.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/watch?v=xgwaz7Z9FIU&pp=ygUHU2lkZW1lbtIHCQm9AIO1pN6f1A%3D%3D', 'https://www.youtube.com/watch?v=B9ou3pu3xSQ&pp=ygUHU2lkZW1lbg%3D%3D']\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool2.run(\"Sidemen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool3 = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tavily_search_results_json'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool3.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool3.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'description': 'search query to look up',\n",
       "  'title': 'Query',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool3.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'How India Is Failing Its Educated Youth - YouTube',\n",
       "  'url': 'https://www.youtube.com/watch?v=cHkFU8BzeDw',\n",
       "  'content': \"How India Is Failing Its Educated Youth \\n Bloomberg Originals \\n 29806 likes \\n 1136136 views \\n 6 Sep 2024 \\n India is the world's fastest growing major economy. Yet, itâ€™s struggling to create jobs for its expanding population. According to research, unemployment for those aged 20 to 24 has risen to over 42%. Itâ€™s a key challenge for Prime Minister Narendra Modi, who hopes to achieve developed economy status for the country by 2047.\",\n",
       "  'score': 0.81979275},\n",
       " {'title': \"Why Can't Young Indians Find Jobs? What is the employment rate in ...\",\n",
       "  'url': 'https://www.outlookbusiness.com/in-depth/why-cant-young-indians-find-jobs',\n",
       "  'content': 'Indiaâ€™s jobs conundrum is a complex, hydra-headed challenge. Not only are the countryâ€™s youth desperate to get jobs, there are also questions of skills shortage, a fetish for sarkari jobs and policy bungling by successive governments that moved the focus away from manufacturing to services among other pain points.\\n\\nPlugging the Hole [...] The root of this issue is laid bare in the Economic Survey 2023-24. Despite 65% of Indiaâ€™s population being under the age of 35, many lack the skills needed for todayâ€™s economy. â€œEstimates show that only about 51.25% of the youth are deemed employable. In other words, one in two young people are not yet ready for the workforce straight out of college,â€ notes the survey, authored by V Anantha Nageswaran, chief economic adviser (CEA) to the Government of India. [...] â€œSeveral factors contribute to Indiaâ€™s shortage, including the prevalence of informal employees, wage issues, educational system constraints, insufficient skill development, migration to other sectors and social stigma associated with manufacturing jobs,â€ he says.',\n",
       "  'score': 0.8095324},\n",
       " {'title': \"How come India doesn't have the culture of teens/young adults ...\",\n",
       "  'url': 'https://www.reddit.com/r/AskIndia/comments/1ebigiw/how_come_india_doesnt_have_the_culture_of/',\n",
       "  'content': \"India has enough population for every job. If the teens were to work, someone would be out of their job. And these gigs don't pay much.\",\n",
       "  'score': 0.74563974},\n",
       " {'title': 'Highly educated Indians are often underemployed â€“ DW â€“ 11/21/2024',\n",
       "  'url': 'https://www.dw.com/en/higher-education-correlates-with-lower-employment-in-india/a-70843565',\n",
       "  'content': 'This year\\'s India Employment Report also shows that nearly 83% of the country\\'s unemployed workforce is\\xa0made up of youth aged 15 to 29.\\xa0Strikingly, nearly two out of three unemployed people in India in 2022 were young and educated â€” meaning they have secondary or higher education.\\n\\nChakraborty believes that this can be partly explained by highly trained youths holding out for a high-status job. [...] Regions\\n\\nTopics\\n\\nCategories\\n\\nHighly educated Indians are often underemployed\\n\\nDespite its massive growing population, India is facing a shortage of skilled workers. Highly educated young people are among the most likely to be jobless.\\n\\nAbout\\xa0half of India\\'s graduates are considered employable, according to a recent survey highlighting the disconnect between the country\\'s rapid development, its massive youth population\\xa0and the needs of its quickly evolving labor market. [...] \"The aspirations for their desired white-collar jobs preempt the educated youth from entering job markets faster,\" added Chakraborty.\\n\\nEducation system \\'needs to be set right\\'\\n\\nArun Kumar, a retired professor of economics at the Jawaharlal Nehru University, told DW that the government has not made sufficient investments in the health and education sectors and has yet to recognize their critical roles in improving national outcomes.',\n",
       "  'score': 0.7409441},\n",
       " {'title': \"Youth unemployment remains one of India's great challenges\",\n",
       "  'url': 'https://www.dandc.eu/en/article/spite-high-growth-rates-masses-indians-are-being-left-behind-and-young-people-particular',\n",
       "  'content': 'In theory, Indiaâ€™s large young workforce should deliver a â€œdemographic dividendâ€. Economists use that term for a situation where masses of young people drive productive growth without having to take care of many old people and children. High youth unemployment, however, means that there is no demographic dividend. According to the ILO report, inadequate education and lacking skills are to blame. The data show that 75\\u2009% cannot send e-mails with attachments, for example, and that 90\\u2009% do not know [...] Singhâ€™s wife is trying to find a job as a teacher at a government school. Such positions are only rarely advertised, however, and when they are, many applicants have the necessary qualifications. To get the job, one must often pay bribes. It is not enough to hand in the appropriate documents. This is the fate of many young Indians in one of the worldâ€™s fastest-growing economies.\\n\\nLabour market development in India [...] When he was first elected in 2014, Modi had attracted young people with the promise of creating 250 million jobs over a decade. Ten years later, it is obvious to everyone that his government failed to make that happen.',\n",
       "  'score': 0.7230167}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool3.invoke({\"query\" : \"Why is India failing to give jobs to youth?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools([\"wikipedia\"],llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct (Reasoning + Acting) Agent\n",
    "agent = initialize_agent(\n",
    "    tool,  # List of tools the agent can use\n",
    "    llm,  # The Language Model that powers the agent\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agent type: \"Zero-Shot ReAct\"\n",
    "    verbose=True  # Enables detailed logging of the agent's thought process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp\\ipykernel_11296\\831658667.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"what is the current empolyement rate in India ?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to look up the current employment rate in India. \n",
      "Action: wikipedia\n",
      "Action Input: India employment rate\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Unemployment in India\n",
      "Summary: Statistics on unemployment in India had traditionally been collected, compiled and disseminated once every ten years by the Ministry of Labour and Employment (MLE), primarily from sample studies conducted by the National Sample Survey Office. Other than these 5-year sample studies, India had historically not collected monthly, quarterly or yearly nationwide employment and unemployment statistics on a routine basis. In 2016, the Centre for Monitoring Indian Economy, a non-governmental entity based in Mumbai, started sampling and publishing monthly unemployment in India statistics. Despite having one of the longest working hours, India has one of the lowest workforce productivity levels in the world. Economists often say that due to structural economic problems, India is experiencing jobless economic growth.\n",
      "\n",
      "Page: List of states and union territories of India by unemployment rate\n",
      "Summary: This is a list of States and union territories of India ranked according to unemployment rate. The list is compiled from the Report on Periodic Labour Force Survey (2018â€“19) released by Ministry of Statistics and Programme Implementation, Government of India.\n",
      "Chhattisgarh has the least unemployment rate among the Indian states, while Rajasthan has the highest unemployment rate. (Higher rank represents higher unemployment among the population). The national average stands at 6.4 percent.\n",
      "\n",
      "\n",
      "\n",
      "Page: Indian labour law\n",
      "Summary: Indian labour law refers to law regulating labour in India. Traditionally, the Indian government at the federal and state levels has sought to ensure a high degree of protection for workers, but in practice, this differs due to the form of government and because labour is a subject in the concurrent list of the Indian Constitution. The Minimum Wages Act 1948 requires companies to pay the minimum wage set by the government alongside limiting working weeks to 40 hours (9 hours a day including an hour of break). Overtime is strongly discouraged with the premium on overtime being 100% of the total wage. The Payment of Wages Act 1936 mandates the payment of wages on time on the last working day of every month via bank transfer or postal service. The Factories Act 1948 and the Shops and Establishment Act 1960 mandate 18 working days of fully paid vacation or earned leaves and 7 casual leaves each year to each employee, with an additional 7 fully paid sick days. The Maternity Benefit (Amendment) Act, 2017 gives female employees of every company the right to take 6 months' worth of fully paid maternity leave. It also provides for 6 weeks worth of paid leaves in case of miscarriage or medical termination of pregnancy. The Employees' Provident Fund Organisation and the Employees' State Insurance, governed by statutory acts provide workers with necessary social security for retirement benefits and medical and unemployment benefits respectively. Workers entitled to be covered under the Employees' State Insurance (those making less than Rs 21000/month) are also entitled to 90 days worth of paid medical leaves. A contract of employment can always provide for more rights than the statutory minimum set rights. The Indian parliament passed four labour codes in the 2019 and 2020 sessions. These four codes will consolidate 44 existing labour laws. They are: The Industrial Relations Code 2020, The Code on Social Security 2020, The Occupational Safety, Health and Working Conditions Code, 2020 and The Code on Wages 2019. Despite having one of the longest working hours, India has one of the lowest workforce productivity levels in the world.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The national average unemployment rate is 6.4 percent. \n",
      "Final Answer: The current employment rate in India is 93.6 percent (100% - 6.4%). \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current employment rate in India is 93.6 percent (100% - 6.4%).'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"what is the current empolyement rate in India ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
